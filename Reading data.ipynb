{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data\n",
    "텐서플로우로 데이터를 로드하는 방법으로는 다음과 같이 3가지가 있다.\n",
    "- <font color='red'>Feeding</font> : 파이썬 코드에서 training step에 데이터를 feed하는 방법\n",
    "- <font color='red'>Reading from files</font> : 텐서플로우 그래프 앞단에서 파일을 읽어오는 input pipline을 구현하는 방법\n",
    "- <font color='red'>Preloaded data</font> : 모든 데이터를 텐서플로우 그래프의 constant나 variable에 저장하는 방법(작은 데이터셋의 경우)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding\n",
    "- 기본적인 데이터 로드 방법\n",
    "- 텐서플로우 그래프에 tf.placeholder 를 선언하고 sess.run()이나 eval()에서 python 데이터(list, np, ..)를 feed함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    input = tf.placeholder(tf.float32)\n",
    "    multiplier = input * 10\n",
    "    print(multiplier.eval(feed_dict={input: 5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from files\n",
    "파일을 읽어오는 input pipeline을 구현하는 단계는 다음과 같다\n",
    "1. The list of filenames\n",
    "2. Optional filename shuffling\n",
    "3. Optional epoch limit\n",
    "4. Filename queue\n",
    "5. A reader for the file format\n",
    "6. A decoder for a record read by the reader\n",
    "7. Optional preprocessing\n",
    "8. Example queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filenames, shuffling, and epoch limits\n",
    "- filenames : \n",
    "    + 파일 이름 리스트, \n",
    "    + [\"file0\", \"file2\"], [(\"file%d\" % i) for i in range(2)]와 같이 표현될 수 있음\n",
    "    + tf.train.match_filenames_once로 불러올 수 있음\n",
    "- tf.train.string_input_producer :\n",
    "    + 파일 이름을 갖는 FIFO queue 생성\n",
    "    + option으로 shuffling, maximum number of epochs 등을 설정할 수 있음\n",
    "    + queue runner는 filenames에 포함된 모든 element를 epoch마다 queue에 추가함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File formats\n",
    "- input file format에 맞추어 Reader생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV files\n",
    "- tf.TextLineReader, tf.decode_csv : CSV(comma-separated value) format 파일 읽기\n",
    "- tf.TextLineReader : test file 읽기\n",
    "- tf.decode_csv : csv파일 디코드\n",
    "    + record_defaults : determines the type of the resulting tensors and sets the default value to use if a value is missing in the input string.\n",
    "\n",
    "- run(), eval() method 전에 반드시 tf.train.start_queue_runners 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer([\"../Species/data/train_labels.csv\"])\n",
    "\n",
    "reader = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "# Default values, in case of empty columns. Also specifies the type of the\n",
    "# decoded result.\n",
    "record_defaults = [[1], [1]]\n",
    "col1, col2 = tf.decode_csv(\n",
    "    value, record_defaults=record_defaults)\n",
    "features = tf.stack([col1])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Start populating the filename queue.\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for i in range(1200):\n",
    "        # Retrieve a single instance:\n",
    "        example, label = sess.run([features, col2])\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed length records\n",
    "- tf.FixedLengthRecodReader, tf.decode_raw : binary 파일의 fixed number of bytes를 읽을때 사용(uint8 tensor로 변환)\n",
    "- 예를들어, CIFAR-10 dataset에서는 시작 1 byte는 label이고 이후 3072 bytes는 image data임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Tensorflow format\n",
    "<p>Another approach is to convert whatever data you have into a supported format.\n",
    "This approach makes it easier to mix and match data sets and network\n",
    "architectures. The recommended format for TensorFlow is a\n",
    "<a href=\"https://www.tensorflow.org/api_guides/python/python_io#tfrecords_format_details\">TFRecords file</a>\n",
    "containing\n",
    "<a href=\"https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/core/example/example.proto\"><code>tf.train.Example</code> protocol buffers</a>\n",
    "(which contain\n",
    "<a href=\"https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/core/example/feature.proto\"><code>Features</code></a>\n",
    "as a field).  You write a little program that gets your data, stuffs it in an\n",
    "<code>Example</code> protocol buffer, serializes the protocol buffer to a string, and then\n",
    "writes the string to a TFRecords file using the\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/python_io/TFRecordWriter\"><code>tf.python_io.TFRecordWriter</code></a>.\n",
    "For example,\n",
    "<a href=\"https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/how_tos/reading_data/convert_to_records.py\"><code>tensorflow/examples/how_tos/reading_data/convert_to_records.py</code></a>\n",
    "converts MNIST data to this format.</p>\n",
    "<p>To read a file of TFRecords, use\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/TFRecordReader\"><code>tf.TFRecordReader</code></a> with\n",
    "the <a href=\"https://www.tensorflow.org/api_docs/python/tf/parse_single_example\"><code>tf.parse_single_example</code></a>\n",
    "decoder. The <code>parse_single_example</code> op decodes the example protocol buffers into\n",
    "tensors. An MNIST example using the data produced by <code>convert_to_records</code> can be\n",
    "found in\n",
    "<a href=\"https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py\"><code>tensorflow/examples/how_tos/reading_data/fully_connected_reader.py</code></a>,\n",
    "which you can compare with the <code>fully_connected_feed</code> version.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "<p>You can then do any preprocessing of these examples you want. This would be any\n",
    "processing that doesn't depend on trainable parameters. Examples include\n",
    "normalization of your data, picking a random slice, adding noise or distortions,\n",
    "etc.  See\n",
    "<a href=\"https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10/cifar10_input.py\"><code>tensorflow_models/tutorials/image/cifar10/cifar10_input.py</code></a>\n",
    "for an example.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching\n",
    "data batch를 위해 training, evaluation, or inference에서 사용할 다른 queue를 생성\n",
    "- tf.train.shuffle_batch : queue that randomizes the order of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_my_file_format(filename_queue):\n",
    "    reader = tf.SomeReader()\n",
    "    key, record_string = reader.read(filename_queue)\n",
    "    example, label = tf.some_decoder(record_string)\n",
    "    processed_example = some_processing(example)\n",
    "    return processed_example, label\n",
    "\n",
    "def input_pipeline(filenames, batch_size, num_epochs=None):\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        filenames, num_epochs=num_epochs, shuffle=True)\n",
    "    example, label = read_my_file_format(filename_queue)\n",
    "    # min_after_dequeue defines how big a buffer we will randomly sample\n",
    "    #   from -- bigger means better shuffling but slower start up and more\n",
    "    #   memory used.\n",
    "    # capacity must be larger than min_after_dequeue and the amount larger\n",
    "    #   determines the maximum we will prefetch.  Recommendation:\n",
    "    #   min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
    "    min_after_dequeue = 10000\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch, label_batch = tf.train.shuffle_batch(\n",
    "        [example, label], batch_size=batch_size, capacity=capacity,\n",
    "        min_after_dequeue=min_after_dequeue)\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>If you need more parallelism or shuffling of examples between files, use\n",
    "multiple reader instances using the\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/train/shuffle_batch_join\"><code>tf.train.shuffle_batch_join</code></a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_pipeline(filenames, batch_size, read_threads, num_epochs=None):\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        filenames, num_epochs=num_epochs, shuffle=True)\n",
    "    example_list = [read_my_file_format(filename_queue)\n",
    "                    for _ in range(read_threads)]\n",
    "    min_after_dequeue = 10000\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch, label_batch = tf.train.shuffle_batch_join(\n",
    "        example_list, batch_size=batch_size, capacity=capacity,\n",
    "        min_after_dequeue=min_after_dequeue)\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>You still only use a single filename queue that is shared by all the readers.\n",
    "That way we ensure that the different readers use different files from the same\n",
    "epoch until all the files from the epoch have been started.  (It is also usually\n",
    "sufficient to have a single thread filling the filename queue.)</p>\n",
    "<p>An alternative is to use a single reader via the\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/train/shuffle_batch\"><code>tf.train.shuffle_batch</code></a>\n",
    "with <code>num_threads</code> bigger than 1.  This will make it read from a single file at\n",
    "the same time (but faster than with 1 thread), instead of N files at once.\n",
    "This can be important:</p>\n",
    "<ul>\n",
    "<li>If you have more reading threads than input files, to avoid the risk that\n",
    "    you will have two threads reading the same example from the same file near\n",
    "    each other.</li>\n",
    "<li>Or if reading N files in parallel causes too many disk seeks.</li>\n",
    "</ul>\n",
    "<p>How many threads do you need? the <code>tf.train.shuffle_batch*</code> functions add a\n",
    "summary to the graph that indicates how full the example queue is. If you have\n",
    "enough reading threads, that summary will stay above zero.  You can\n",
    "<a href=\"https://www.tensorflow.org/get_started/summaries_and_tensorboard\">view your summaries as training progresses using TensorBoard</a>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create threads to prefetch using QueueRunner objects\n",
    "앞서 소개된 대부분의 tf.train 함수들은 tf.train.QueueRunner objects를 사용함\n",
    "- 따라서 training이나 inference들을 수행하기 전에 tf.train.start_queue_runners를 수행해야 함\n",
    "- 쓰래드 관리를 위해서 tf.train.Coordinator를 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example\n",
    "w = tf.Variable([5], dtype=tf.float32)\n",
    "b = tf.constant([5], dtype=tf.float32)\n",
    "train_op = w + b\n",
    "\n",
    "# Create the graph, etc.\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# Create a session for running operations in the Graph.\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialize the variables (like the epoch counter).\n",
    "sess.run(init_op)\n",
    "\n",
    "# Start input enqueue threads.\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "try:\n",
    "    #while not coord.should_stop():\n",
    "    for i in range(10)\n",
    "        # Run training steps or whatever\n",
    "        sess.run(train_op)\n",
    "\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print('Done training -- epoch limit reached')\n",
    "finally:\n",
    "    # When done, ask the threads to stop.\n",
    "    coord.request_stop()\n",
    "\n",
    "# Wait for threads to finish.\n",
    "coord.join(threads)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: What is happening here?\n",
    "그래프를 생성하면, queue들로 연결된 pipeline stages가 생성됨\n",
    "1. read를 위한 filenames을 생성하고, filename queue에 enqueue operation 수행\n",
    "2. Reader를 통해 filename queue에서 filenames를 가져오고, examples를 생성하고, example queue에 enqueue operation 수행\n",
    "3. training 단계에서 dequeue oepration이 수행되면, enqueue operation이 수행됨\n",
    "    - threads가 enqueuing operation을 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:70%; margin-left:12%; margin-bottom:10px; margin-top:20px;\">\n",
    "<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/AnimatedFileQueues.gif\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The helpers in <code>tf.train</code> that create these queues and enqueuing operations add\n",
    "a <a href=\"https://www.tensorflow.org/api_docs/python/tf/train/QueueRunner\"><code>tf.train.QueueRunner</code></a> to the\n",
    "graph using the\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/train/add_queue_runner\"><code>tf.train.add_queue_runner</code></a>\n",
    "function. Each <code>QueueRunner</code> is responsible for one stage, and holds the list of\n",
    "enqueue operations that need to be run in threads. Once the graph is\n",
    "constructed, the\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/train/start_queue_runners\"><code>tf.train.start_queue_runners</code></a>\n",
    "function asks each QueueRunner in the graph to start its threads running the\n",
    "enqueuing operations.</p>\n",
    "<p>If all goes well, you can now run your training steps and the queues will be\n",
    "filled by the background threads. If you have set an epoch limit, at some point\n",
    "an attempt to dequeue examples will get an\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/errors/OutOfRangeError\"><code>tf.errors.OutOfRangeError</code></a>. This\n",
    "is the TensorFlow equivalent of \"end of file\" (EOF) -- this means the epoch\n",
    "limit has been reached and no more examples are available.</p>\n",
    "<p>The last ingredient is the\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/train/Coordinator\"><code>tf.train.Coordinator</code></a>. This is responsible\n",
    "for letting all the threads know if anything has signalled a shut down. Most\n",
    "commonly this would be because an exception was raised, for example one of the\n",
    "threads got an error when running some operation (or an ordinary Python\n",
    "exception).</p>\n",
    "<p>For more about threading, queues, QueueRunners, and Coordinators\n",
    "<a href=\"https://www.tensorflow.org/programmers_guide/threading_and_queues\">see here</a>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside: How clean shut-down when limiting epochs works\n",
    "<p>Imagine you have a model that has set a limit on the number of epochs to train\n",
    "on.  That means that the thread generating filenames will only run that many\n",
    "times before generating an <code>OutOfRange</code> error. The QueueRunner will catch that\n",
    "error, close the filename queue, and exit the thread. Closing the queue does two\n",
    "things:</p>\n",
    "<ul>\n",
    "<li>Any future attempt to enqueue in the filename queue will generate an error.\n",
    "    At this point there shouldn't be any threads trying to do that, but this\n",
    "    is helpful when queues are closed due to other errors.</li>\n",
    "<li>Any current or future dequeue will either succeed (if there are enough\n",
    "    elements left) or fail (with an <code>OutOfRange</code> error) immediately.  They won't\n",
    "    block waiting for more elements to be enqueued, since by the previous point\n",
    "    that can't happen.</li>\n",
    "</ul>\n",
    "<p>The point is that when the filename queue is closed, there will likely still be\n",
    "many filenames in that queue, so the next stage of the pipeline (with the reader\n",
    "and other preprocessing) may continue running for some time.  Once the filename\n",
    "queue is exhausted, though, the next attempt to dequeue a filename (e.g. from a\n",
    "reader that has finished with the file it was working on) will trigger an\n",
    "<code>OutOfRange</code> error.  In this case, though, you might have multiple threads\n",
    "associated with a single QueueRunner.  If this isn't the last thread in the\n",
    "QueueRunner, the <code>OutOfRange</code> error just causes the one thread to exit.  This\n",
    "allows the other threads, which are still finishing up their last file, to\n",
    "proceed until they finish as well.  (Assuming you are using a\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/train/Coordinator\"><code>tf.train.Coordinator</code></a>,\n",
    "other types of errors will cause all the threads to stop.)  Once all the reader\n",
    "threads hit the <code>OutOfRange</code> error, only then does the next queue, the example\n",
    "queue, gets closed.</p>\n",
    "<p>Again, the example queue will have some elements queued, so training will\n",
    "continue until those are exhausted.  If the example queue is a\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/RandomShuffleQueue\"><code>tf.RandomShuffleQueue</code></a>, say\n",
    "because you are using <code>shuffle_batch</code> or <code>shuffle_batch_join</code>, it normally will\n",
    "avoid ever having fewer than its <code>min_after_dequeue</code> attr elements buffered.\n",
    "However, once the queue is closed that restriction will be lifted and the queue\n",
    "will eventually empty.  At that point the actual training threads, when they\n",
    "try and dequeue from example queue, will start getting <code>OutOfRange</code> errors and\n",
    "exiting.  Once all the training threads are done,\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/train/Coordinator#join\"><code>tf.train.Coordinator.join</code></a>\n",
    "will return and you can exit cleanly.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering records or producing multiple examples per recode\n",
    "<p>Instead of examples with shapes <code>[x, y, z]</code>, you will produce a batch of\n",
    "examples with shape <code>[batch, x, y, z]</code>.  The batch size can be 0 if you want to\n",
    "filter this record out (maybe it is in a hold-out set?), or bigger than 1 if you\n",
    "are producing multiple examples per record.  Then simply set <code>enqueue_many=True</code>\n",
    "when calling one of the batching functions (such as <code>shuffle_batch</code> or\n",
    "<code>shuffle_batch_join</code>).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse input data\n",
    "<p>SparseTensors don't play well with queues. If you use SparseTensors you have\n",
    "to decode the string records using\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/parse_example\"><code>tf.parse_example</code></a> <strong>after</strong>\n",
    "batching (instead of using <code>tf.parse_single_example</code> before batching).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preloaded data\n",
    "모든 데이터를 memory에 로드하기 때문에 작은 데이터셋에 한정에서 사용함\n",
    "- Preloaded data에는 다음과 같이 2가지 approach가 있음\n",
    "    + constant로 저장\n",
    "    + variable로 저장한 후, never change\n",
    "    \n",
    "constant를 사용하는 것은 simple하지만 memory를 많이 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = ...\n",
    "training_labels = ...\n",
    "with tf.Session():\n",
    "    input_data = tf.constant(training_data)\n",
    "    input_labels = tf.constant(training_labels)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable을 사용하는 경우에는 initialize 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = ...\n",
    "training_labels = ...\n",
    "with tf.Session() as sess:\n",
    "    data_initializer = tf.placeholder(dtype=training_data.dtype,\n",
    "                                    shape=training_data.shape)\n",
    "    label_initializer = tf.placeholder(dtype=training_labels.dtype,\n",
    "                                     shape=training_labels.shape)\n",
    "    input_data = tf.Variable(data_initializer, trainable=False, collections=[])\n",
    "    input_labels = tf.Variable(label_initializer, trainable=False, collections=[])\n",
    "    ...\n",
    "    sess.run(input_data.initializer,\n",
    "           feed_dict={data_initializer: training_data})\n",
    "    sess.run(input_labels.initializer,\n",
    "           feed_dict={label_initializer: training_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Setting <code>trainable=False</code> keeps the variable out of the\n",
    "<code>GraphKeys.TRAINABLE_VARIABLES</code> collection in the graph, so we won't try and\n",
    "update it when training.  Setting <code>collections=[]</code> keeps the variable out of the\n",
    "<code>GraphKeys.GLOBAL_VARIABLES</code> collection used for saving and restoring checkpoints.</p>\n",
    "<p>Either way,\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/train/slice_input_producer\"><code>tf.train.slice_input_producer</code></a>\n",
    "can be used to produce a slice at a time.  This shuffles the examples across an\n",
    "entire epoch, so further shuffling when batching is undesirable.  So instead of\n",
    "using the <code>shuffle_batch</code> functions, we use the plain\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/train/batch\"><code>tf.train.batch</code></a> function.  To use\n",
    "multiple preprocessing threads, set the <code>num_threads</code> parameter to a number\n",
    "bigger than 1.</p>\n",
    "<p>An MNIST example that preloads the data using constants can be found in\n",
    "<a href=\"https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/how_tos/reading_data/fully_connected_preloaded.py\"><code>tensorflow/examples/how_tos/reading_data/fully_connected_preloaded.py</code></a>, and one that preloads the data using variables can be found in\n",
    "<a href=\"https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/how_tos/reading_data/fully_connected_preloaded_var.py\"><code>tensorflow/examples/how_tos/reading_data/fully_connected_preloaded_var.py</code></a>,\n",
    "You can compare these with the <code>fully_connected_feed</code> and\n",
    "<code>fully_connected_reader</code> versions above.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple input pipeline\n",
    "<p>Commonly you will want to train on one dataset and evaluate (or \"eval\") on\n",
    "another.  One way to do this is to actually have two separate processes:</p>\n",
    "<ul>\n",
    "<li>The training process reads training input data and periodically writes\n",
    "  checkpoint files with all the trained variables.</li>\n",
    "<li>The evaluation process restores the checkpoint files into an inference\n",
    "  model that reads validation input data.</li>\n",
    "</ul>\n",
    "<p>This is what is done in\n",
    "<a href=\"https://www.tensorflow.org/tutorials/deep_cnn#save_and_restore_checkpoints\">the example CIFAR-10 model</a>.  This has a couple of benefits:</p>\n",
    "<ul>\n",
    "<li>The eval is performed on a single snapshot of the trained variables.</li>\n",
    "<li>You can perform the eval even after training has completed and exited.</li>\n",
    "</ul>\n",
    "<p>You can have the train and eval in the same graph in the same process, and share\n",
    "their trained variables.  See\n",
    "<a href=\"https://www.tensorflow.org/programmers_guide/variable_scope\">the shared variables tutorial</a>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Tensorflow Reading data : https://www.tensorflow.org/programmers_guide/reading_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
